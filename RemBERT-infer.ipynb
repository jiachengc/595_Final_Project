{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n    \n<h3><span \"style: color=#444\">Introduction</span></h3>\n\nThe kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n\nThis is a three part kernel,\n\n- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n\n- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n\n- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n\n<h3><span \"style: color=#444\">Techniques</span></h3>\n\nThe kernel has implementation for below techniques, click on the links to learn more -\n\n - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n \n - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n \n - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n \n - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n \n - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n \n - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n \n - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n \n - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n \n - etc.\n \n<h3><span \"style: color=#444\">References</span></h3>\nI would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n\n- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n\n- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n\n- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n\n- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n\n- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read.","metadata":{}},{"cell_type":"markdown","source":"# 不得不看的QA例子(question and answer)\nhttps://blog.csdn.net/qq_42388742/article/details/113843510\n\n# transformer tokenizerAPI使用\nhttps://zhuanlan.zhihu.com/p/390821442\n\n# fine-tuning\nhttps://zhuanlan.zhihu.com/p/390823624\n","metadata":{}},{"cell_type":"markdown","source":"<h3><span style=\"color=#444\">Note</span></h3>\n\nThe below points are worth noting,\n\n - I haven't used FP16 because due to some reason this fails and model never starts training.\n - These are the original hyperparamters and setting that I have used for training my models.\n - I tried few pooling layers but none of them performed better than simple one.\n - Gradient clipping reduces model performance.\n - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>","metadata":{}},{"cell_type":"markdown","source":"### Install APEX","metadata":{}},{"cell_type":"markdown","source":"# 使用Apex进行混合混合精度训练\n混合精度训练，即组合浮点数 (FP32)和半精度浮点数 (FP16)进行训练，允许我们使用更大的batchsize，并利用NVIDIA张量核进行更快的计算。","metadata":{}},{"cell_type":"code","source":"# %%writefile setup.sh\n# export CUDA_HOME=/usr/local/cuda-10.1\n# git clone https://github.com/NVIDIA/apex\n# cd apex\n# pip install -v --cuda_ext --cpp_ext --disable-pip-version-check --no-cache-dir ./","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","outputId":"99831be8-d7e0-47d4-c561-f266d3ed1fd3","execution":{"iopub.status.busy":"2021-08-21T23:10:00.179601Z","iopub.execute_input":"2021-08-21T23:10:00.180081Z","iopub.status.idle":"2021-08-21T23:10:00.19012Z","shell.execute_reply.started":"2021-08-21T23:10:00.180045Z","shell.execute_reply":"2021-08-21T23:10:00.188949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%capture\n# !sh setup.sh","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-21T23:10:00.191722Z","iopub.execute_input":"2021-08-21T23:10:00.191986Z","iopub.status.idle":"2021-08-21T23:10:01.577036Z","shell.execute_reply.started":"2021-08-21T23:10:00.19196Z","shell.execute_reply":"2021-08-21T23:10:01.575901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Dependencies","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing#\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim#优化器\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler#分布式采样器\n# 混合精度训练\n# https://blog.csdn.net/weixin_40248634/article/details/107941930?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163785683016780269866777%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163785683016780269866777&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-107941930.pc_search_es_clickV2&utm_term=apex%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6&spm=1018.2226.3001.4187\ntry:\n    from torch.cuda import amp#混合精度训练\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\n# http://fancyerii.github.io/2020/07/08/huggingface-transformers/\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\n# https://www.jianshu.com/p/f2d0dbdc51c9÷\nlogging.set_verbosity_warning()#设置输出级别：\nlogging.set_verbosity_error()#设置输出级别\n#随机种子设置\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n#\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"id":"E4l6PirHET3x","outputId":"eeaea823-bdbc-4bef-a518-e51736877d6e","execution":{"iopub.status.busy":"2021-11-25T16:28:36.62664Z","iopub.execute_input":"2021-11-25T16:28:36.627308Z","iopub.status.idle":"2021-11-25T16:28:43.7454Z","shell.execute_reply.started":"2021-11-25T16:28:36.627205Z","shell.execute_reply":"2021-11-25T16:28:43.743718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Configuration\n\n### 训练参数设置\n- https://github.com/datawhalechina/learn-nlp-with-transformers datawhalwe开源的NLP transfomer学习资源","metadata":{}},{"cell_type":"code","source":"\n# https://huggingface.co/deepset/xlm-roberta-large-squad2   预训练模型的官方文档\n# https://zhuanlan.zhihu.com/p/390821442                    tokenizer文档\n\nclass Config:\n    # model\n    model_type = 'xlm_roberta'#muril.  rembert\n    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n    config_name = \"deepset/xlm-roberta-large-squad2\"\n    \n    #fp16。16位精度训练，是混合精度训练的相关设置https://blog.csdn.net/weixin_40248634/article/details/107941930?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163785683016780269866777%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163785683016780269866777&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-107941930.pc_search_es_clickV2&utm_term=apex%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6&spm=1018.2226.3001.4187\n    fp16 = False\n    fp16_opt_level = \"O1\"#欧1\n    gradient_accumulation_steps = 2\n\n    \n#所以我们可以发现，tokenizer 帮我们处理了所有，\n# 对文本进行特殊字符的添加\n# padding\n# truncation\n# encoding （tokenize，convert_tokens_to_ids）\n# 转化为tensor\n# 输出 model 需要的attention mask\n# （optional) 以及输出 token type ids\n\n    \n    # tokenizer#分词\n    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n    max_seq_length = 400#最大序列长度（也叫序列长度，因为长度不够的会被填充）\n    doc_stride = 135#允许的重叠长度，因为超长的时候文本会被截断，就是一个句子会被裁减成两个，但是有时候答案可能会在裁剪处被分成两部分，因此允许裁剪的地方允许存在重叠\n    ####裁剪举例\n    #原文123456789987654321 max_seq_length = 12 理论上裁剪后123456789987  654321000000  但是答案可能是8765 这样答案就被分为了两个部分\n    #使用了填充后可能就是123456789987。 789987654321，这样答案就不会被分割开\n\n    # train\n    epochs = 2\n    train_batch_size = 4\n    eval_batch_size = 8\n\n    # optimizer\n#     https://blog.csdn.net/kyle1314608/article/details/100589449?ops_request_misc=&request_id=&biz_id=102&utm_term=adamw%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-100589449.pc_search_es_clickV2&spm=1018.2226.3001.4187\n    optimizer_type = 'AdamW'\n    learning_rate = 1.5e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n#     (一)、什么是Warmup?\n# Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。\n\n# (二)、为什么使用Warmup?\n# 由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。\n\n\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 1234","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-21T23:10:08.645152Z","iopub.execute_input":"2021-08-21T23:10:08.645674Z","iopub.status.idle":"2021-08-21T23:10:08.652283Z","shell.execute_reply.started":"2021-08-21T23:10:08.645633Z","shell.execute_reply":"2021-08-21T23:10:08.651484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Factory\n### 做了数据kfold，打上了fold标签，后续根据fold标签对数据分割","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nexternal_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])\n#数据分折\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n        data.loc[v_, 'kfold'] = f\n    return data\n\ntrain = create_folds(train, num_splits=5)\nexternal_train[\"kfold\"] = -1\nexternal_train['id'] = list(np.arange(1, len(external_train)+1))\ntrain = pd.concat([train, external_train]).reset_index(drop=True)\n\ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}\n\ntrain['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-21T23:10:08.654573Z","iopub.execute_input":"2021-08-21T23:10:08.65482Z","iopub.status.idle":"2021-08-21T23:10:09.843527Z","shell.execute_reply.started":"2021-08-21T23:10:08.654795Z","shell.execute_reply":"2021-08-21T23:10:09.842643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Covert Examples to Features (Preprocess)","metadata":{}},{"cell_type":"code","source":"def prepare_train_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()#.lstrip(),截掉字符串坐标的空格\n    #tokenizer    https://zhuanlan.zhihu.com/p/390821442\n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",#文本过长进行截断，onlysecond是只对context进行切割，切割question是我们不想的\n        max_length=args.max_seq_length,#编码长度\n        stride=args.doc_stride,#允许的重叠长度\n        return_overflowing_tokens=True,#返回重叠的部分，就是跟上面重叠部分长度搭配使用的\n        return_offsets_mapping=True,#我们需要知道我们编码里面哪个是正确答案以及特征的具体位置，map码，00000011111000000，用一标记我们的答案。自己看文档\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n    features = []\n    #数据处理，下面通俗易懂就是问答任务的匹配问题\n    \n#     https://blog.csdn.net/qq_42388742/article/details/113843510\n    for i, offsets in enumerate(offset_mapping):\n        feature = {}\n\n        input_ids = tokenized_example[\"input_ids\"][i]\n        attention_mask = tokenized_example[\"attention_mask\"][i]\n\n        feature['input_ids'] = input_ids\n        feature['attention_mask'] = attention_mask\n        feature['offset_mapping'] = offsets\n\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_example.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answers = example[\"answers\"]\n\n        if len(answers[\"answer_start\"]) == 0:\n            feature[\"start_position\"] = cls_index\n            feature[\"end_position\"] = cls_index\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                feature[\"start_position\"] = token_start_index - 1\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                feature[\"end_position\"] = token_end_index + 1\n\n        features.append(feature)\n    return features","metadata":{"id":"dxbZdct1ET3z","execution":{"iopub.status.busy":"2021-08-21T23:10:09.844781Z","iopub.execute_input":"2021-08-21T23:10:09.845137Z","iopub.status.idle":"2021-08-21T23:10:09.858766Z","shell.execute_reply.started":"2021-08-21T23:10:09.845103Z","shell.execute_reply":"2021-08-21T23:10:09.85676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Retriever\n## 数据处理","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"id":"6TuzHdjmET30","execution":{"iopub.status.busy":"2021-08-21T23:10:09.860362Z","iopub.execute_input":"2021-08-21T23:10:09.86082Z","iopub.status.idle":"2021-08-21T23:10:09.872411Z","shell.execute_reply.started":"2021-08-21T23:10:09.860781Z","shell.execute_reply":"2021-08-21T23:10:09.871443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        #预训练模型。\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)#dropout层，最后的全链接层使用\n        #dropout layer的目的是为了防止CNN 过拟合。那么为什么可以有效的防止过拟合呢？\n\n# 首先，想象我们现在只训练一个特定的网络，当迭代次数增多的时候，可能出现网络对训练集拟合的很好（在训练集上loss很小），\n# 但是对验证集的拟合程度很差的情况。所以，我们有了这样的想法：可不可以让每次跌代随机的去更新网络参数（weights），\n# 引入这样的随机性就可以增加网络generalize 的能力。所以就有了dropout 。\n\n        self._init_weights(self.qa_outputs)\n    #初始化权重\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n#     forward函数\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n        # token_type_ids=None\n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","metadata":{"id":"9OxhKqxcET31","execution":{"iopub.status.busy":"2021-08-21T23:10:09.876001Z","iopub.execute_input":"2021-08-21T23:10:09.876367Z","iopub.status.idle":"2021-08-21T23:10:09.885872Z","shell.execute_reply.started":"2021-08-21T23:10:09.876331Z","shell.execute_reply":"2021-08-21T23:10:09.884876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss","metadata":{}},{"cell_type":"code","source":"def loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    \n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    total_loss = (start_loss + end_loss) / 2\n    return total_loss","metadata":{"id":"SxuNrJqqET32","execution":{"iopub.status.busy":"2021-08-21T23:10:09.887871Z","iopub.execute_input":"2021-08-21T23:10:09.888262Z","iopub.status.idle":"2021-08-21T23:10:09.899699Z","shell.execute_reply.started":"2021-08-21T23:10:09.888214Z","shell.execute_reply":"2021-08-21T23:10:09.89886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grouped Layerwise Learning Rate Decay","metadata":{}},{"cell_type":"code","source":"# 分层训练\ndef get_optimizer_grouped_parameters(args, model):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n    ]\n    return optimizer_grouped_parameters","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-21T23:10:09.900857Z","iopub.execute_input":"2021-08-21T23:10:09.902966Z","iopub.status.idle":"2021-08-21T23:10:09.9187Z","shell.execute_reply.started":"2021-08-21T23:10:09.902935Z","shell.execute_reply":"2021-08-21T23:10:09.917835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metric Logger","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","metadata":{"id":"bkFB-iMcET34","execution":{"iopub.status.busy":"2021-08-21T23:10:09.920476Z","iopub.execute_input":"2021-08-21T23:10:09.921136Z","iopub.status.idle":"2021-08-21T23:10:09.933984Z","shell.execute_reply.started":"2021-08-21T23:10:09.921015Z","shell.execute_reply":"2021-08-21T23:10:09.933061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#预训练模型\n# model\n# tokenizer\n# config\ndef make_model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = Model(args.model_name_or_path, config=config)\n    return config, tokenizer, model\n#优化器定义\ndef make_optimizer(args, model):\n#     optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    if args.optimizer_type == \"AdamW\":\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=args.learning_rate,\n            eps=args.epsilon,\n            correct_bias=True\n        )\n        return optimizer\n\ndef make_scheduler(\n    args, optimizer, \n    num_warmup_steps, \n    num_training_steps\n):\n    if args.decay_name == \"cosine-warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    return scheduler    \n\ndef make_loader(\n    args, data, \n    tokenizer, fold\n):\n    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n    \n    train_features, valid_features = [[] for _ in range(2)]\n    for i, row in train_set.iterrows():\n        train_features += prepare_train_features(args, row, tokenizer)\n    for i, row in valid_set.iterrows():\n        valid_features += prepare_train_features(args, row, tokenizer)\n\n    train_dataset = DatasetRetriever(train_features)\n    valid_dataset = DatasetRetriever(valid_features)\n    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.train_batch_size,\n        sampler=train_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=valid_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n\n    return train_dataloader, valid_dataloader","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-21T23:10:09.937073Z","iopub.execute_input":"2021-08-21T23:10:09.937469Z","iopub.status.idle":"2021-08-21T23:10:09.95418Z","shell.execute_reply.started":"2021-08-21T23:10:09.93744Z","shell.execute_reply":"2021-08-21T23:10:09.95325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self, model, tokenizer, \n        optimizer, scheduler\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train(\n        self, args, \n        train_dataloader, \n        epoch, result_dict\n    ):\n        count = 0\n        losses = AverageMeter()\n        \n        self.model.zero_grad()\n        self.model.train()\n        \n        fix_all_seeds(args.seed)\n        \n        for batch_idx, batch_data in enumerate(train_dataloader):\n            input_ids, attention_mask, targets_start, targets_end = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['start_position'], batch_data['end_position']\n            \n            input_ids, attention_mask, targets_start, targets_end = \\\n                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n\n            outputs_start, outputs_end = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            )\n            \n            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n            loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            count += input_ids.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n\n            # if args.fp16:\n            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n            # else:\n            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n\n            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n\n            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n                _s = str(len(str(len(train_dataloader.sampler))))\n                ret = [\n                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n                    'Train Loss: {: >4.5f}'.format(losses.avg),\n                ]\n                print(', '.join(ret))\n\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-21T23:10:09.957274Z","iopub.execute_input":"2021-08-21T23:10:09.957665Z","iopub.status.idle":"2021-08-21T23:10:09.974148Z","shell.execute_reply.started":"2021-08-21T23:10:09.957635Z","shell.execute_reply":"2021-08-21T23:10:09.97318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluator","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model):\n        self.model = model\n    \n    def save(self, result, output_dir):\n        with open(f'{output_dir}/result_dict.json', 'w') as f:\n            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n\n    def evaluate(self, valid_dataloader, epoch, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch_data in enumerate(valid_dataloader):\n            self.model = self.model.eval()\n            input_ids, attention_mask, targets_start, targets_end = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['start_position'], batch_data['end_position']\n            \n            input_ids, attention_mask, targets_start, targets_end = \\\n                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n            \n            with torch.no_grad():            \n                outputs_start, outputs_end = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                )\n                \n                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n                losses.update(loss.item(), input_ids.size(0))\n                \n        print('----Validation Results Summary----')\n        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-21T23:10:09.977392Z","iopub.execute_input":"2021-08-21T23:10:09.977696Z","iopub.status.idle":"2021-08-21T23:10:09.987493Z","shell.execute_reply.started":"2021-08-21T23:10:09.977667Z","shell.execute_reply":"2021-08-21T23:10:09.986541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Training","metadata":{}},{"cell_type":"code","source":"def init_training(args, data, fold):\n    fix_all_seeds(args.seed)\n    \n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    # model\n    model_config, tokenizer, model = make_model(args)\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n    \n    # data loaders\n    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n\n    # optimizer\n    optimizer = make_optimizer(args, model)\n\n    # scheduler\n    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n    if args.warmup_ratio > 0:\n        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n    else:\n        num_warmup_steps = 0\n    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n\n    # mixed precision training with NVIDIA Apex\n    if args.fp16:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    \n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'best_val_loss': np.inf\n    }\n\n    return (\n        model, model_config, tokenizer, optimizer, scheduler, \n        train_dataloader, valid_dataloader, result_dict\n    )","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-21T23:10:09.989069Z","iopub.execute_input":"2021-08-21T23:10:09.989665Z","iopub.status.idle":"2021-08-21T23:10:10.001441Z","shell.execute_reply.started":"2021-08-21T23:10:09.989625Z","shell.execute_reply":"2021-08-21T23:10:10.000579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{}},{"cell_type":"code","source":"def run(data, fold):\n    args = Config()\n    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n        valid_dataloader, result_dict = init_training(args, data, fold)\n    \n    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n    evaluator = Evaluator(model)\n\n    train_time_list = []\n    valid_time_list = []\n\n    for epoch in range(args.epochs):\n        result_dict['epoch'].append(epoch)\n\n        # Train\n        torch.cuda.synchronize()\n        tic1 = time.time()\n        result_dict = trainer.train(\n            args, train_dataloader, \n            epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic2 = time.time() \n        train_time_list.append(tic2 - tic1)\n        \n        # Evaluate\n        torch.cuda.synchronize()\n        tic3 = time.time()\n        result_dict = evaluator.evaluate(\n            valid_dataloader, epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic4 = time.time() \n        valid_time_list.append(tic4 - tic3)\n            \n        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n            \n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n            model_config.save_pretrained(output_dir)\n            tokenizer.save_pretrained(output_dir)\n            print(f\"Saving model checkpoint to {output_dir}.\")\n            \n        print()\n\n    evaluator.save(result_dict, output_dir)\n    \n    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n    \n    torch.cuda.empty_cache()\n    del trainer, evaluator\n    del model, model_config, tokenizer\n    del optimizer, scheduler\n    del train_dataloader, valid_dataloader, result_dict\n    gc.collect()","metadata":{"id":"39ei5Bm5ET37","execution":{"iopub.status.busy":"2021-08-21T23:10:10.003025Z","iopub.execute_input":"2021-08-21T23:10:10.003443Z","iopub.status.idle":"2021-08-21T23:10:10.01816Z","shell.execute_reply.started":"2021-08-21T23:10:10.003378Z","shell.execute_reply":"2021-08-21T23:10:10.017273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(1):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n    print('-'*50)\n    run(train, fold)","metadata":{"id":"mPaGnnCnbhWl","outputId":"1b25d103-7d4d-4779-ea74-34312b42ad4c","execution":{"iopub.status.busy":"2021-08-21T23:10:10.019515Z","iopub.execute_input":"2021-08-21T23:10:10.02003Z","iopub.status.idle":"2021-08-21T23:13:39.819256Z","shell.execute_reply.started":"2021-08-21T23:10:10.019991Z","shell.execute_reply":"2021-08-21T23:13:39.817583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#example for training second fold\n\nfor fold in range(1, 2):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n    print('-'*50)\n    run(train, fold)","metadata":{"id":"DkjRIhdbwjHx","outputId":"c62d2d45-cd44-4ca9-a775-3814971090d7","execution":{"iopub.status.busy":"2021-08-21T23:13:39.820621Z","iopub.status.idle":"2021-08-21T23:13:39.821368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(2, 3):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n    print('-'*50)\n    run(train, fold)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:13:39.822621Z","iopub.status.idle":"2021-08-21T23:13:39.823381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(3, 4):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n    print('-'*50)\n    run(train, fold)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:13:39.824537Z","iopub.status.idle":"2021-08-21T23:13:39.825291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(4, 5):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n    print('-'*50)\n    run(train, fold)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:13:39.82638Z","iopub.status.idle":"2021-08-21T23:13:39.827106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Thanks and please do Upvote!","metadata":{}}]}